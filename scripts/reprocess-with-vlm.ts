/**
 * VLMìœ¼ë¡œ ê¸°ì¡´ ë‰´ìŠ¤ ì´ìŠˆ ì¬ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
 *
 * ì‚¬ìš©ë²•: npx tsx scripts/reprocess-with-vlm.ts
 */

import * as dotenv from 'dotenv'
dotenv.config({ path: '.env.local' })

import { createClient } from '@supabase/supabase-js'
import { extractStructuredWithVLM } from '../lib/news-extractor'

const supabase = createClient(
  (process.env.SUPABASE_URL || process.env.NEXT_PUBLIC_SUPABASE_URL)!,
  process.env.SUPABASE_SERVICE_KEY!
)

interface NewsPage {
  id: number
  issue_id: number
  page_number: number
  image_url: string
  ocr_text: string | null
  ocr_provider: string | null
}

interface NewsIssue {
  id: number
  issue_date: string
  status: string
}

async function reprocessAllIssues() {
  console.log('\n' + '='.repeat(70))
  console.log('ğŸ”„ VLMìœ¼ë¡œ ê¸°ì¡´ ë‰´ìŠ¤ ì´ìŠˆ ì¬ì²˜ë¦¬')
  console.log('='.repeat(70))

  // 1. ì™„ë£Œëœ ì´ìŠˆ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
  const { data: issues, error: issuesError } = await supabase
    .from('news_issues')
    .select('id, issue_date, status')
    .eq('status', 'completed')
    .order('issue_date', { ascending: false })

  if (issuesError || !issues) {
    console.error('âŒ ì´ìŠˆ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨:', issuesError?.message)
    return
  }

  console.log(`\nğŸ“‹ ì¬ì²˜ë¦¬ ëŒ€ìƒ: ${issues.length}ê°œ ì´ìŠˆ`)
  for (const issue of issues) {
    console.log(`   - ${issue.issue_date} (ID: ${issue.id})`)
  }

  // 2. ê° ì´ìŠˆ ì¬ì²˜ë¦¬
  for (const issue of issues) {
    await reprocessIssue(issue)
  }

  console.log('\n' + '='.repeat(70))
  console.log('âœ… ëª¨ë“  ì´ìŠˆ ì¬ì²˜ë¦¬ ì™„ë£Œ!')
  console.log('='.repeat(70))
}

async function reprocessIssue(issue: NewsIssue) {
  console.log(`\n${'â”€'.repeat(70)}`)
  console.log(`ğŸ“° ${issue.issue_date} ì¬ì²˜ë¦¬ ì¤‘...`)
  console.log(`${'â”€'.repeat(70)}`)

  // 1. í•´ë‹¹ ì´ìŠˆì˜ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
  const { data: pages, error: pagesError } = await supabase
    .from('news_pages')
    .select('id, issue_id, page_number, image_url, ocr_text, ocr_provider')
    .eq('issue_id', issue.id)
    .order('page_number', { ascending: true })

  if (pagesError || !pages || pages.length === 0) {
    console.log(`   âš ï¸ í˜ì´ì§€ ì—†ìŒ: ${pagesError?.message}`)
    return
  }

  console.log(`   ğŸ“„ í˜ì´ì§€ ìˆ˜: ${pages.length}`)

  // 2. ê¸°ì¡´ articlesì™€ chunks ì‚­ì œ
  console.log('   ğŸ—‘ï¸ ê¸°ì¡´ articles/chunks ì‚­ì œ ì¤‘...')

  await supabase
    .from('news_chunks')
    .delete()
    .eq('issue_id', issue.id)

  await supabase
    .from('news_articles')
    .delete()
    .eq('issue_id', issue.id)

  // 3. ê° í˜ì´ì§€ VLMìœ¼ë¡œ ì¬ì²˜ë¦¬
  let totalArticles = 0
  let totalChunks = 0
  let totalCorrections = 0

  for (const page of pages) {
    console.log(`\n   ğŸ“„ í˜ì´ì§€ ${page.page_number} ì²˜ë¦¬ ì¤‘...`)

    if (!page.image_url) {
      console.log(`      âš ï¸ ì´ë¯¸ì§€ URL ì—†ìŒ, ê±´ë„ˆëœ€`)
      continue
    }

    try {
      // ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
      const response = await fetch(page.image_url)
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}`)
      }
      const imageBuffer = Buffer.from(await response.arrayBuffer())
      console.log(`      âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: ${(imageBuffer.length / 1024).toFixed(1)}KB`)

      // VLM ì¶”ì¶œ
      const contentType = page.image_url.toLowerCase().includes('.png') ? 'image/png' : 'image/jpeg'
      const vlmResult = await extractStructuredWithVLM(imageBuffer, contentType)

      console.log(`      âœ… VLM ì¶”ì¶œ ì™„ë£Œ: ${vlmResult.provider}`)
      console.log(`         ê¸°ì‚¬ ìˆ˜: ${vlmResult.data.articles?.length || 0}`)
      console.log(`         êµì • ìˆ˜: ${vlmResult.corrections.length}`)

      totalCorrections += vlmResult.corrections.length

      // OCR í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ì „ì²´ í…ìŠ¤íŠ¸ë¡œ)
      const fullText = vlmResult.data.articles?.map(a =>
        `[${a.title}]\n${a.content}`
      ).join('\n\n') || ''

      await supabase
        .from('news_pages')
        .update({
          ocr_text: fullText,
          ocr_provider: `VLM-${vlmResult.provider}`,
        })
        .eq('id', page.id)

      // ê¸°ì‚¬ ì €ì¥
      if (vlmResult.data.articles && vlmResult.data.articles.length > 0) {
        for (const article of vlmResult.data.articles) {
          // news_articlesì— ì €ì¥
          const { data: savedArticle, error: articleError } = await supabase
            .from('news_articles')
            .insert({
              issue_id: issue.id,
              page_id: page.id,
              title: article.title || 'ì œëª© ì—†ìŒ',
              content: article.content || '',
              article_type: article.type || 'article',
            })
            .select('id')
            .single()

          if (articleError) {
            console.log(`      âš ï¸ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: ${articleError.message}`)
            continue
          }

          totalArticles++

          // news_chunksì— ì €ì¥ (ê¸°ì‚¬ ë‚´ìš©ì„ ì²­í¬ë¡œ)
          if (article.content && article.content.length > 0) {
            // ê¸´ ë‚´ìš©ì€ ì²­í¬ë¡œ ë¶„í•  (500ì ê¸°ì¤€)
            const chunks = splitIntoChunks(article.content, 500)

            for (let i = 0; i < chunks.length; i++) {
              const { error: chunkError } = await supabase
                .from('news_chunks')
                .insert({
                  issue_id: issue.id,
                  article_id: savedArticle?.id,
                  page_number: page.page_number,
                  issue_date: issue.issue_date,
                  article_title: article.title || 'ì œëª© ì—†ìŒ',
                  article_type: article.type || 'article',
                  chunk_text: chunks[i],
                  chunk_index: i,
                })

              if (!chunkError) {
                totalChunks++
              }
            }
          }
        }
      }

    } catch (pageError: any) {
      console.log(`      âŒ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: ${pageError.message}`)
    }

    // API ì†ë„ ì œí•œ ë°©ì§€
    await new Promise(resolve => setTimeout(resolve, 2000))
  }

  console.log(`\n   ğŸ“Š ${issue.issue_date} ê²°ê³¼:`)
  console.log(`      ê¸°ì‚¬: ${totalArticles}ê°œ`)
  console.log(`      ì²­í¬: ${totalChunks}ê°œ`)
  console.log(`      êµì •: ${totalCorrections}ê±´`)
}

function splitIntoChunks(text: string, maxLength: number): string[] {
  const chunks: string[] = []
  const paragraphs = text.split(/\n\n+/)
  let currentChunk = ''

  for (const para of paragraphs) {
    if (currentChunk.length + para.length + 2 <= maxLength) {
      currentChunk += (currentChunk ? '\n\n' : '') + para
    } else {
      if (currentChunk) {
        chunks.push(currentChunk.trim())
      }
      // ê¸´ ë‹¨ë½ì€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
      if (para.length > maxLength) {
        const sentences = para.split(/(?<=[.!?ã€‚])\s+/)
        currentChunk = ''
        for (const sentence of sentences) {
          if (currentChunk.length + sentence.length + 1 <= maxLength) {
            currentChunk += (currentChunk ? ' ' : '') + sentence
          } else {
            if (currentChunk) {
              chunks.push(currentChunk.trim())
            }
            currentChunk = sentence
          }
        }
      } else {
        currentChunk = para
      }
    }
  }

  if (currentChunk.trim()) {
    chunks.push(currentChunk.trim())
  }

  return chunks.length > 0 ? chunks : [text]
}

// ë‹¨ì¼ ì´ìŠˆë§Œ ì¬ì²˜ë¦¬
async function reprocessSingleIssue(issueId: number) {
  const { data: issue, error } = await supabase
    .from('news_issues')
    .select('id, issue_date, status')
    .eq('id', issueId)
    .single()

  if (error || !issue) {
    console.error('âŒ ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:', error?.message)
    return
  }

  await reprocessIssue(issue)
}

// CLI ì¸ì ì²˜ë¦¬
const args = process.argv.slice(2)
if (args.includes('--issue') && args.indexOf('--issue') + 1 < args.length) {
  const issueId = parseInt(args[args.indexOf('--issue') + 1])
  reprocessSingleIssue(issueId).catch(console.error)
} else {
  reprocessAllIssues().catch(console.error)
}
